{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55351eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import email\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "#sklearn libraries\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b16f712",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb004444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_file_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000/000</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000/001</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000/002</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000/003</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000/004</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data_file_path label\n",
       "0        000/000   ham\n",
       "1        000/001  spam\n",
       "2        000/002  spam\n",
       "3        000/003   ham\n",
       "4        000/004  spam"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import dataset and the stop words\n",
    "#put items from the labels file to the labels dictionary separating the label and its corresponding data file\n",
    "email_labels_dict = {'data_file_path':[], 'label':[]}\n",
    "\n",
    "with open(\"trec06/trec06p-cs280/labels\") as labels:\n",
    "    for line in labels:\n",
    "        #splitting ham/spam and the data file path item in labels file. Example: label= ham and file= ../data/000/000\n",
    "        label, file = line.split()\n",
    "        if label == 'spam':\n",
    "            email_labels_dict['data_file_path'].append(file.replace(\"../data/\",\"\"))\n",
    "            email_labels_dict['label'].append('spam')\n",
    "        elif label == 'ham':\n",
    "            email_labels_dict['data_file_path'].append(file.replace(\"../data/\",\"\"))\n",
    "            email_labels_dict['label'].append('ham')\n",
    "            \n",
    "#convert the email_label_dict dictionary as a dataframe             \n",
    "emails = pd.DataFrame.from_dict(email_labels_dict)\n",
    "emails.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b909589e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'able', 'about', 'above', 'abst', 'accordance']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import stop words from stop_words file and put these words in an array.\n",
    "\n",
    "with open('stop_words.txt','r') as words:\n",
    "    stop_words = words.read().splitlines()\n",
    "\n",
    "#display only a few elements in the stop_words. Just enough to show that each words are separated in the array\n",
    "stop_words[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34c2340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words and other unecessary characters from email content\n",
    "\n",
    "email_message = []\n",
    "\n",
    "def clean_email_content(content):\n",
    "    cleaned_words = []\n",
    "    words = content.split()\n",
    "    \n",
    "    for word in words:\n",
    "        uncleaned_word = re.sub(r'<[^<>]+>','', word)\n",
    "        uncleaned_word = re.sub(r'[^a-zA-Z\\n ]','', uncleaned_word)\n",
    "        uncleaned_word = re.sub(r'http\\S+', '', uncleaned_word)\n",
    "        uncleaned_word = re.sub(r'www', '', uncleaned_word)\n",
    "        uncleaned_word = re.sub(r'goo\\S+', '', uncleaned_word)\n",
    "        \n",
    "        if uncleaned_word not in stop_words:\n",
    "            cleaned_words.append(uncleaned_word)\n",
    "\n",
    "    cleaned_content = list(filter(None, cleaned_words))\n",
    "    str_cleaned_content = ' '.join(cleaned_content)\n",
    "    return str_cleaned_content\n",
    "    \n",
    "    \n",
    "# go through the files in data\n",
    "for file_path in emails['data_file_path']:\n",
    "    \n",
    "    # open the file with encoiding set to ISO-8859-1 \n",
    "    with open(f\"trec06/trec06p-cs280/data/{file_path}\", \"r\", encoding=\"ISO-8859-1\") as email_content:\n",
    "        message = \"\"   \n",
    "        email_msg = email.message_from_file(email_content)\n",
    "\n",
    "        #if emails are multipart\n",
    "        if email_msg.is_multipart():\n",
    "            #loop through the email's parts\n",
    "            for part in email_msg.walk():\n",
    "                #if the content-type is text/plain then get content\n",
    "                if part.get_content_type() == \"text/plain\": \n",
    "                    message = part.get_payload() \n",
    "                    break\n",
    "        #if the email is not a multipart\n",
    "        else:\n",
    "            message = email_msg.get_payload()\n",
    "        \n",
    "        #remove stop words and unnecessary characters in email content\n",
    "        cleaned_msg= clean_email_content(message)\n",
    "        \n",
    "        #the cleaned message is converted to lowercase before being appended to the email_message list\n",
    "        email_message.append(cleaned_msg.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00527f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_file_path</th>\n",
       "      <th>label</th>\n",
       "      <th>email_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000/000</td>\n",
       "      <td>ham</td>\n",
       "      <td>the mailing list i queried weeks ago running i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000/001</td>\n",
       "      <td>spam</td>\n",
       "      <td>luxury watches buy your own rolex for only rol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000/002</td>\n",
       "      <td>spam</td>\n",
       "      <td>academic qualifications prestigious nonacc red...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000/003</td>\n",
       "      <td>ham</td>\n",
       "      <td>greetings this verify subscription planfans li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000/004</td>\n",
       "      <td>spam</td>\n",
       "      <td>chauncey conferred luscious continued tonsillitis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data_file_path label                                      email_content\n",
       "0        000/000   ham  the mailing list i queried weeks ago running i...\n",
       "1        000/001  spam  luxury watches buy your own rolex for only rol...\n",
       "2        000/002  spam  academic qualifications prestigious nonacc red...\n",
       "3        000/003   ham  greetings this verify subscription planfans li...\n",
       "4        000/004  spam  chauncey conferred luscious continued tonsillitis"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails['email_content'] = email_message\n",
    "\n",
    "emails.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46270c92",
   "metadata": {},
   "source": [
    "## Splitting datasets for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bcf34d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training set is:  (21300, 3)\n",
      "The shape of testing set is:  (16522, 3)\n"
     ]
    }
   ],
   "source": [
    "#train datasets. Folders 0-70\n",
    "email_train = emails[emails['data_file_path'] < '071']\n",
    "\n",
    "#test datasets. Folders 0\n",
    "email_test = emails[emails['data_file_path'] >= '071']\n",
    "\n",
    "#check if shape is corect base on what is stated in the instructions\n",
    "print(\"The shape of the training set is: \", email_train.shape)\n",
    "print(\"The shape of testing set is: \", email_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "167a467c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training set is:  (12910, 4)\n",
      "The shape of testing set is:  (24912, 4)\n"
     ]
    }
   ],
   "source": [
    "#Split training dataset for ham and spam\n",
    "\n",
    "ham_train = emails[emails['label'] == 'ham'].reset_index()\n",
    "spam_train = emails[emails['label']== 'spam'].reset_index()\n",
    "\n",
    "print(\"The shape of the training set is: \", ham_train.shape)\n",
    "print(\"The shape of testing set is: \", spam_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18542790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>num_of_occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "      <td>37617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bb</td>\n",
       "      <td>18521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>14554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>12342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>td</td>\n",
       "      <td>11367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  words  num_of_occurences\n",
       "0     i              37617\n",
       "1    bb              18521\n",
       "2   the              14554\n",
       "3     a              12342\n",
       "4    td              11367"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract the most common (1000) words from the training dataset\n",
    "most_used_words = Counter(\" \".join(email_train['email_content']).split()).most_common(1000)\n",
    "\n",
    "top_used_words = pd.DataFrame(most_used_words, columns = ['words', 'num_of_occurences'])\n",
    "top_used_words.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce1bfda",
   "metadata": {},
   "source": [
    "## Creating the feature matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2a8545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the feature dictionary for ham and spam\n",
    "ham_word_counts = {unique_words: [0] * len(ham_train) for unique_words, _ in most_used_words}\n",
    "spam_word_counts = {unique_words: [0] * len(spam_train) for unique_words, _ in most_used_words}\n",
    "\n",
    "top_words_list = [key for key, _ in most_used_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eda5a8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [3, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spam Feature Set\n",
    "spam_word_count = pd.DataFrame(spam_word_counts)\n",
    "\n",
    "#loop through the spam train set index\n",
    "for i in spam_train.index:\n",
    "        # count the word frequency per row in the train ham set\n",
    "        frequency = dict(Counter(spam_train['email_content'][i].split()))\n",
    "        for key, val in frequency.items():\n",
    "            if key in top_words_list:\n",
    "                spam_word_count.loc[i, key] += val\n",
    "\n",
    "spam_feat_matrix = spam_word_count.to_numpy()\n",
    "spam_feat_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35ec246b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ham Feature Set\n",
    "ham_word_count = pd.DataFrame(ham_word_counts)\n",
    "\n",
    "for i in ham_train.index:\n",
    "        frequency = dict(Counter(spam_train['email_content'][i].split()))\n",
    "        for key, val in frequency.items():\n",
    "            if key in top_words_list:  # add the word frequency to the row and column where the word is found\n",
    "                ham_word_count.loc[i, key] += val\n",
    "\n",
    "ham_feat_matrix = ham_word_count.to_numpy()\n",
    "ham_feat_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f689df0",
   "metadata": {},
   "source": [
    "## Computing priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "095a0263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior probabilities for spam:  1.1695774647887325\n",
      "Prior probabilities for ham:  0.6061032863849766\n"
     ]
    }
   ],
   "source": [
    "spam_train_size = spam_train.shape[0]        #number of spam emails\n",
    "ham_train_size = ham_train.shape[0]          #number of ham emails\n",
    "total_train_size = email_train.shape[0]      #total number of emails for training\n",
    "\n",
    "spam_prior =  spam_train_size / total_train_size\n",
    "ham_prior = ham_train_size / total_train_size\n",
    "\n",
    "print(\"Prior probabilities for spam: \", spam_prior)\n",
    "print(\"Prior probabilities for ham: \", ham_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eef019",
   "metadata": {},
   "source": [
    "## Computing the Likelihood of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0316ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum of each words in spam\n",
    "spam_train_words_sum = np.sum(spam_feat_matrix, axis=0)\n",
    "#sum of each words in ham\n",
    "ham_train_words_sum = np.sum(ham_feat_matrix, axis=0)\n",
    "\n",
    "#total sum of words in spam\n",
    "spam_train_word_total = spam_train_words_sum.sum()\n",
    "#total sum of words in ham\n",
    "ham_train_word_total = ham_train_words_sum.sum()\n",
    "\n",
    "#spelled as lamda because lambda is a given function\n",
    "lamda = 1 #laplace smoothing\n",
    "ham_likelihood = {}\n",
    "spam_likelihood = {}\n",
    "#formula based on the given formula in the instructions\n",
    "\n",
    "for i in range(len(top_words_list)):\n",
    "    \n",
    "    c_ham = (ham_train_words_sum[i]+lamda) / (ham_train_word_total + lamda*len(top_words_list))\n",
    "    c_spam = (spam_train_words_sum[i]+lamda) / (spam_train_word_total + lamda*len(top_words_list))\n",
    "    \n",
    "    ham_likelihood[top_words_list[i]] = c_ham\n",
    "    spam_likelihood[top_words_list[i]] = c_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98a7905",
   "metadata": {},
   "source": [
    "## Classifying Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74d7fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for classifying the emails\n",
    "\n",
    "def classifying_emails(email_content, spam_prior, ham_prior, spam_likelihood, ham_likelihood, top_words_list):\n",
    "    \n",
    "    #get log values of spam and ham probabilities\n",
    "    spam_prob_log = np.log(spam_prior)\n",
    "    ham_prob_log = np.log(ham_prior)\n",
    "    \n",
    "    email_words = str(email_content).split()\n",
    "    \n",
    "    for word in email_words:\n",
    "\n",
    "        if word in top_words_list:\n",
    "            ham_prob_log += np.log(ham_likelihood[word])\n",
    "            spam_prob_log += np.log(spam_likelihood[word])\n",
    "    \n",
    "    if spam_prob_log > ham_prob_log:\n",
    "        return \"spam\"\n",
    "    else:\n",
    "        return \"ham\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9196e56f",
   "metadata": {},
   "source": [
    "## Testing the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a6dd225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary with the predicted label with the corresponding data file path\n",
    "predicted_dict = {'data_file_path':[], 'predicted_label': []}\n",
    "\n",
    "for path, content in zip(email_test['data_file_path'], email_test['email_content']):\n",
    "    predicted_dict['data_file_path'].append(path) \n",
    "    classification = classifying_emails(content, spam_prior, ham_prior, spam_likelihood, ham_likelihood, top_words_list)\n",
    "    #add the predicted label by the classfying_emails function\n",
    "    predicted_dict['predicted_label'].append(classification) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "550dd226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_file_path</th>\n",
       "      <th>label</th>\n",
       "      <th>email_content</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>071/000</td>\n",
       "      <td>spam</td>\n",
       "      <td>where hesitantly derive perverse satisfaction ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>071/001</td>\n",
       "      <td>ham</td>\n",
       "      <td>there things perform experiment first display ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>071/002</td>\n",
       "      <td>spam</td>\n",
       "      <td>best offer month viggra ci ialis vaiium xa naa...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>071/003</td>\n",
       "      <td>spam</td>\n",
       "      <td>de ar home o wne your cr doesnt matter if ow n...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>071/004</td>\n",
       "      <td>spam</td>\n",
       "      <td>special offer adobe video collection adobe pre...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16517</th>\n",
       "      <td>126/017</td>\n",
       "      <td>spam</td>\n",
       "      <td>great news expec ted infinex ventures inc infx...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16518</th>\n",
       "      <td>126/018</td>\n",
       "      <td>spam</td>\n",
       "      <td>the oil sector going crazy this weekly gift ge...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16519</th>\n",
       "      <td>126/019</td>\n",
       "      <td>spam</td>\n",
       "      <td>suffering pain depression heartburn well help ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16520</th>\n",
       "      <td>126/020</td>\n",
       "      <td>spam</td>\n",
       "      <td>u n i v e r s i t y d i p l o m a s do prosper...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16521</th>\n",
       "      <td>126/021</td>\n",
       "      <td>spam</td>\n",
       "      <td>moat coverall cytochemistry planeload salk</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16522 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      data_file_path label                                      email_content  \\\n",
       "0            071/000  spam  where hesitantly derive perverse satisfaction ...   \n",
       "1            071/001   ham  there things perform experiment first display ...   \n",
       "2            071/002  spam  best offer month viggra ci ialis vaiium xa naa...   \n",
       "3            071/003  spam  de ar home o wne your cr doesnt matter if ow n...   \n",
       "4            071/004  spam  special offer adobe video collection adobe pre...   \n",
       "...              ...   ...                                                ...   \n",
       "16517        126/017  spam  great news expec ted infinex ventures inc infx...   \n",
       "16518        126/018  spam  the oil sector going crazy this weekly gift ge...   \n",
       "16519        126/019  spam  suffering pain depression heartburn well help ...   \n",
       "16520        126/020  spam  u n i v e r s i t y d i p l o m a s do prosper...   \n",
       "16521        126/021  spam         moat coverall cytochemistry planeload salk   \n",
       "\n",
       "      predicted_label  \n",
       "0                spam  \n",
       "1                spam  \n",
       "2                spam  \n",
       "3                 ham  \n",
       "4                 ham  \n",
       "...               ...  \n",
       "16517             ham  \n",
       "16518             ham  \n",
       "16519            spam  \n",
       "16520             ham  \n",
       "16521            spam  \n",
       "\n",
       "[16522 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test = pd.DataFrame.from_dict(predicted_dict)\n",
    "\n",
    "merged_emails = pd.merge(email_test, predicted_test, on='data_file_path')\n",
    "merged_emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76b7b5a",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b14faa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test set accuracy score: 0.5430940564096356\n",
      "The test set recall score: 0.7900314324202964\n",
      "The test set precision score: 0.6279982866933181\n",
      "The test set F1 score: 0.6997573877421152\n"
     ]
    }
   ],
   "source": [
    "#performance evaluation for accuracy, recall, precision and f1_score.\n",
    "true_label = merged_emails['label'].to_numpy()\n",
    "predicted_label = merged_emails['predicted_label'].to_numpy()\n",
    "print(\"The test set accuracy score:\", accuracy_score(true_label, predicted_label))\n",
    "print(\"The test set recall score:\", recall_score(true_label, predicted_label, pos_label=\"spam\"))\n",
    "print(\"The test set precision score:\", precision_score(true_label, predicted_label, pos_label=\"spam\"))\n",
    "print(\"The test set F1 score:\", f1_score(true_label, predicted_label, pos_label=\"spam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a98095f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8797, 2338],\n",
       "       [5211,  176]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion_matrix\n",
    "c_matrix = confusion_matrix(merged_emails['label'].to_numpy(), merged_emails['predicted_label'].to_numpy(), labels=[\"spam\", \"ham\"])\n",
    "c_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e339d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
